# -*- coding: utf-8 -*-
"""Water_Quality_Classification_Fix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q798VXbe98__iJCP1eeUl79JF8Ys1sAK

# Proyek Machine Learning & Deep Learning - Water Quality Classification

Dataset yang digunakan dalam proyek _machine learning_ ini adalah "Water Quality and Potability," yang tersedia di platform [Kaggle](https://www.kaggle.com/datasets/uom190346a/water-quality-and-potability). _Dataset_ ini merupakan kumpulan data kuantitatif yang mencakup berbagai kolom untuk menentukan apakah air layak diminum atau tidak. Secara keseluruhan, dataset ini terdiri dari 3276 baris dan 10 kolom.

Dataset_ ini sangat sesuai untuk membangun model _supervised learning_, khususnya untuk tugas _binary classification_. Dalam konteks ini, model akan digunakan untuk mengklasifikasikan sampel air menjadi dua kategori: layak diminum atau tidak layak diminum

# Data Diri

- Nama : Royan Sabila Rosyad W
- Id Dicoding : royanrosyad85
- Institusi : UIN Syarif Hidayatullah Jakarta

# Importing Library
"""

# Commented out IPython magic to ensure Python compatibility.
# Meng-import semua library yang digunakan
import pandas as pd
import numpy as np
import tensorflow as tf
import keras
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import missingno as msno
import warnings
from sklearn import metrics
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE, RandomOverSampler

from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score,f1_score, roc_curve, auc
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model,load_model
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
# %matplotlib inline

"""# Importing Dataset"""

# Upload kaggle.json yang didapatkan dari akun Kaggle
from google.colab import files
files.upload()  # Pilih file kaggle.json

# Buat direktori dan ubah izin file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset
!kaggle datasets download -d devanshibavaria/water-potability-dataset-with-10-parameteres

# Unzip dataset dan hapus file zip
!unzip water-potability-dataset-with-10-parameteres.zip

"""Dataset sudah diimport dan sudah diekstrak"""

# Membuat dataset menjadi dataframe dengan nama 'dataset'
df = pd.read_csv('//content//water_potability.csv')

# Preview dataframe dari dataset yang sudah dibuat
df

"""## Understand And Organize the Dataset

Berikut ini adalah informasi lainnya mengenai variabel-variabel yang terdapat di dataset tersebut:

Variabel-variabel pada _Dataset "Water Quality and Potability"_ adalah sebagai berikut:
- ```pH```: Tingkat pH air.
- ```Hardness```: Ukuran kandungan mineral.
- ```Solids```: Total padatan terlarut dalam air.
- ```Chloramines```: Konsentrasi kloramin dalam air.
- ```Sulfate```: Konsentrasi sulfat dalam air.
- ```Conductivity```: Konduktivitas listrik di air.
- ```Organic_carbon```: Kandungan karbon organik dalam air.
- ```Trihalomethanes```: Konsentrasi trihalometan dalam air.
- ```Turbidity```: Tingkat kekeruhan, ukuran kejernihan air.
- ```Potability```: Variabel target. menunjukkan potabilitas air dengan nilai 1 (layak minum) dan 0 (tidak layak minum).

Exploratory Data Analysis (EDA) adalah pendekatan analisis data yang bertujuan untuk memahami karakteristik utama dari kumpulan data. EDA melibatkan penggunaan teknik statistik dan visualisasi grafis untuk menemukan pola, hubungan, atau anomali untuk membentuk hipotesis. Proses ini sering kali tidak terstruktur dan dianggap sebagai langkah awal penting dalam analisis data yang membantu menentukan arah analisis lebih lanjut.
"""

# Menjumlah total missing value pada dataset
df.isnull().sum()

"""Berdasarkan output diatas, terdapat 3 kolom yang memiliki missing value dengan jumlah yang berbeda. Kolom tersebut yaitu:
- ```ph```
- ```Sulfate```
- ```Trihalomethanes```

Jumlah missing value dalam kumpulan data ini terlalu banyak. Daripada menghapusnya, pendekatan yang lebih masuk akal adalah menggantinya dengan nilai median, sehingga data tetap lebih representatif dan tidak kehilangan informasi penting.
"""

df.fillna(df.median(), inplace=True)
df.isnull().sum()

# Menampilan jumlah baris dan kolom yang ada pada dataset

df.shape

"""Berdasarkan output diatas, didapatkan informasi:
  - Terdapat 3276 baris data
  - Tedapat 10 kolom
"""

# Menampilkan kolom-kolom yang ada pada dataset

df.keys()

"""Berdasarkan output diatas, didapatkan informasi:

Dataset memiliki 10 kolom, yaitu 'ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity', 'Potability'

 terdiri dari 1 kolom label ("Potability") 9 kolom feature (sisanya selain kolom label)
"""

# Menampilkan tipe data dari setiap kolom yang ada

df.info()

"""Berdasarkan hasil diatas, didatkan informasi:
- Kolom ```Potability``` memiliki tipe data ```int64```
- Kolom lainnya memiliki tipe data ```float64```
"""

# Menampilkan statistika deskriptif unuk setiap kolom

df.describe().T

"""Fungsi diatas memberikan informasi statistika deskriptif untuk setiap kolom yang ada, yaitu:
- ```count``` : Jumlah data dari sebuah kolom
- ```mean``` : Rata-rata dari sebuah kolom
- ```std``` : Standar deviasi dari sebuah kolom
- ```min``` : Nilai terendah pada sebuah kolom
- ```25%``` : Nilai kuartil pertama (Q1) dari sebuah kolom
- ```50%``` : Nilai kuartil kedua (Q2) atau median atau nilai tengah dari sebuah kolom
- ```75%``` : Nilai kuartil ketiha (Q3) dari sebuah kolom
- ```max``` : Nilai tertinggi pada sebuah kolom
"""

# Cek baris duplikat dalam df
duplicates = df.duplicated()

# Hitung jumlah baris duplikat
duplicate_count = duplicates.sum()

# Cetak jumlah baris duplikat
print(f"Number of duplicate rows: {duplicate_count}")

"""## Exploratory Data Analysis

Exploratory Data Analysis (EDA) adalah pendekatan dalam analisis data yang bertujuan untuk menggali dan memahami karakteristik utama dari sebuah dataset. EDA memanfaatkan teknik statistik dan visualisasi grafis untuk mengungkap pola, hubungan, atau anomali dalam data. Hasil dari proses ini dapat digunakan untuk menyusun hipotesis dan sering kali menjadi langkah awal yang krusial dalam analisis data untuk menentukan arah penelitian selanjutnya.

### Univariate Analysis

Univariate Analysis adalah jenis analisis data yang memeriksa satu variabel (atau bidang data) pada satu waktu. Tujuannya adalah untuk menggambarkan data dan menemukan pola yang ada dalam distribusi variabel tersebut. Ini termasuk penggunaan statistik deskriptif, histogram, dan box plots untuk menganalisis distribusi dan memahami sifat dari variabel tersebut.
"""

# Membuat count plot
sns.countplot(x='Potability', data=df, color='#30D5C8')
sns.despine()
plt.title('Count Plot for Categorical Column')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

"""Berdasarkan *count_plot* diatas, didapatkan informasi bahwa:
- kelas `0` pada label `Potability` memiliki nyaris 2000 data.
- kelas `1` pada label `Potability` hanya memiliki sekitar 1250 data.
"""

# Create a more refined visualization for numerical features
fig, axs = plt.subplots(3, 3, figsize=(15, 10))
axs = axs.flatten()

# Select only numerical columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Create histograms for each numerical column
for i, column in enumerate(numerical_columns[:9]):  # Only take up to 9 columns to fit the grid
    sns.histplot(df[column], bins=20, kde=True, color='#30D5C8', ax=axs[i], alpha=0.7)
    axs[i].set_title(f'Distribution of {column}', fontsize=12)
    axs[i].set_xlabel(column, fontsize=10)
    axs[i].set_ylabel('Frequency', fontsize=10)

    # Add mean line
    mean_val = df[column].mean()
    axs[i].axvline(mean_val, color='red', linestyle='--', linewidth=1.5)
    axs[i].text(mean_val*1.1, axs[i].get_ylim()[1]*0.9, f'Mean: {mean_val:.2f}',
                color='red', fontsize=9)

# Remove unused subplots if any
for i in range(len(numerical_columns[:9]), len(axs)):
    fig.delaxes(axs[i])

plt.suptitle('Distribution of Numerical Features', fontsize=16, y=0.98)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

"""Berdasarkan visualisasi diatas, gambar ini menampilkan setiap kolom numerik yang ada pada dataset, seperti ```pH```, ```Hardness```, ```Solids```, ```Chrolamines```, ```Sulfate```, ```Conductivity```, ```Organic_carbon```, ```Trihalomethanes```, ```Turbidity```. Dari semua kolom yang ditampilkan, hanya kolom ```Solids``` dan ```Conductivity``` yang memiliki skewness ke arah kiri.

### Box Plots
"""

fig, ax = plt.subplots(3, 3, figsize=(15, 15))
fig.subplots_adjust(hspace=0.5, wspace=0.3)

for i, col in enumerate(df.columns[:-1]): #potability column except
    row = i // 3
    col_idx = i % 3

    sns.boxplot(data=df, x='Potability', y=col, ax=ax[row, col_idx],
                flierprops=dict(markerfacecolor='red', marker='o'))
    ax[row, col_idx].set_title(f'Box Plot of {col} by Potability', fontweight='bold')
    ax[row, col_idx].grid(True)

plt.show()

"""Box plot ini memvisualisasikan distribusi setiap fitur numerik (seperti pH, Hardness, Solids, dll.) yang dipisahkan berdasarkan kelas Potability (0: Tidak Layak Minum, 1: Layak Minum). Titik-titik merah menunjukkan adanya outliers (pencilan) pada hampir semua fitur untuk kedua kelas. Secara umum, distribusi (median, rentang interkuartil, dan sebaran data) untuk air yang layak minum (1) dan tidak layak minum (0) tampak sangat mirip di sebagian besar fitur, menunjukkan bahwa tidak ada satu fitur pun yang secara individual memiliki perbedaan distribusi yang jelas antara kedua kelas, yang mengindikasikan hubungan yang lemah antara masing-masing fitur ini dengan kelayakan minum air.

### Correlation Between Numerical Variables
"""

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='YlGnBu', center=0, linewidths=0.5, linecolor='white', fmt='.3f')
plt.title('Correlation Between Numerical Variables')
plt.show()

"""Heatmap korelasi ini menampilkan koefisien korelasi Pearson antar variabel numerik, dengan warna biru tua menandakan korelasi positif yang lebih kuat dan warna terang menunjukkan korelasi lemah atau negatif. Secara keseluruhan, hubungan linier antar fitur tampak lemah, dengan korelasi terkuat yang teramati hanya -0.150 antara Sulfate dan Solids. Variabel target Potability juga menunjukkan korelasi yang sangat rendah dengan semua fitur lainnya, mengimplikasikan bahwa hubungan non-linier atau interaksi antar fitur kemungkinan lebih berperan dalam menentukan kelayakan minum air, sehingga mendukung penggunaan model yang lebih kompleks.

### Multivariate Analysis

_Multivariate Analysis_ adalah prosedur statistik yang digunakan untuk memeriksa hubungan antara beberapa variabel secara bersamaan. Teknik ini mencakup berbagai metode seperti regresi berganda, analisis faktor, dan analisis kluster, yang membantu dalam memahami struktur dan pola yang kompleks dalam data dengan lebih dari satu variabel.
"""

sns.pairplot(df, diag_kind='kde', hue='Potability', height=2.5)
plt.suptitle('Pairplot of Features by Potability', y=1.02, fontsize=16)
plt.show()

"""Berdasarkan hasil visualisasi di atas, tampak bahwa hampir semua variabel terdistribusi di sekitar nilai tengah dan tidak memperlihatkan pola atau karakteristik khusus terhadap variabel label, yaitu ```'Potability'```. Bahkan pada visualisasi tersebut meskipun data telah dipisahkan berdasarkan kategori ```0``` dan ```1``` (dengan warna biru dan oranye), tetap tidak ditemukan pola atau ciri khas tertentu pada masing-masing nilai label. Hal ini menunjukkan bahwa hubungan antar fitur, termasuk dengan variabel label, cenderung lemah atau berkorelasi rendah.

### Correlation

Uji Korelasi adalah metode statistik yang digunakan untuk menentukan apakah ada hubungan antara dua variabel kuantitatif dan seberapa kuat hubungan tersebut. Uji ini menghasilkan nilai koefisien korelasi, seperti Pearson atau Spearman, yang berkisar antara -1 hingga +1. Nilai mendekati +1 menunjukkan korelasi positif yang kuat, sedangkan nilai mendekati -1 menunjukkan korelasi negatif yang kuat. Nilai mendekati 0 menunjukkan tidak adanya korelasi. Uji korelasi penting dalam menentukan arah dan kekuatan hubungan antar variabel, yang dapat membantu dalam pemodelan prediktif dan analisis penyebab.
"""

# Visualisasi korelasi antara fitur numerik dan label Potability

plt.figure(figsize=(8, 12))
corr_matrix = df.corr()[['Potability']].sort_values(by='Potability', ascending=False)
ax = sns.heatmap(corr_matrix, vmin=-1, vmax=1, annot=True, cmap='coolwarm')
ax.set_title('Heatmap Korelasi Fitur terhadap Potability', fontsize=18, pad=16)
plt.show()

"""Berdasarkan  visualisasi diatas, terlihat bahwa kolom ```pH```, ```Conductivity```, ```Trihalomethanes```, ```Turbidity``` memiliki skor korelasi yang paling kecil terhadap label. Kolom yang semacam ini baiknya di-drop saja untuk meringankan beban komputasi dan mengurangi dimensi dari dataset yang akan digunakan dalam pelatihan model"""

for num_row in df.columns[:-1]:
    sns.histplot(data=df,x=num_row,hue='Potability', multiple="stack",palette="YlGnBu",)
    plt.title(f"{num_row} Histogram with Potability")
    plt.xlabel(f"{num_row}")
    plt.ylabel("frequency")
    plt.show()

"""# Data Preparation

### Oversampling to Handle Imbalanced Data

Oversampling adalah teknik yang digunakan untuk menangani masalah ketidakseimbangan kelas dalam dataset. Ketidakseimbangan ini terjadi ketika jumlah data pada satu kelas jauh lebih sedikit dibandingkan kelas lainnya, yang dapat menyebabkan model menjadi bias terhadap kelas mayoritas. Dalam oversampling, data dari kelas minoritas ditambahkan secara sintetis atau diduplikasi hingga jumlahnya seimbang dengan kelas mayoritas. Salah satu metode populer adalah RandomOverSampler, yang menduplikasi data kelas minoritas secara acak, atau SMOTE, yang menciptakan sampel sintetis berdasarkan interpolasi data kelas minoritas.
"""

# Memisahkan fitur dan target
x = df.drop('Potability', axis=1).values
y = df['Potability'].values

# Melihat distribusi kelas awal
count_0 = np.sum(y == 0)
count_1 = np.sum(y == 1)
print("Sebelum oversampling:")
print(f"Jumlah baris data yang bernilai '0' ada sebanyak: {count_0}")
print(f"Jumlah baris data yang bernilai '1' ada sebanyak: {count_1}")
print(f"Persentase kelas 0: {count_0/(count_0+count_1)*100:.2f}%")
print(f"Persentase kelas 1: {count_1/(count_0+count_1)*100:.2f}%")

# Over-sampling dengan keseimbangan penuh
over_sampler = RandomOverSampler(sampling_strategy='auto', random_state=42)
x_resampled, y_resampled = over_sampler.fit_resample(x, y)

# Melihat distribusi kelas setelah oversampling
count_0 = np.sum(y_resampled == 0)
count_1 = np.sum(y_resampled == 1)
print("\nSetelah oversampling:")
print(f"Jumlah baris data yang bernilai '0' ada sebanyak: {count_0}")
print(f"Jumlah baris data yang bernilai '1' ada sebanyak: {count_1}")
print(f"Persentase kelas 0: {count_0/(count_0+count_1)*100:.2f}%")
print(f"Persentase kelas 1: {count_1/(count_0+count_1)*100:.2f}%")

"""Dataset kini memiliki distribusi yang seimbang, dengan masing-masing kelas memiliki jumlah data yang sama (50% untuk kelas 0 dan 50% untuk kelas 1).
Dengan distribusi yang seimbang, model diharapkan dapat memberikan prediksi yang lebih adil dan tidak bias terhadap salah satu kelas.
"""

# Scaling fitur setelah oversampling
scaler = MinMaxScaler((-1, 1))
x_resampled = scaler.fit_transform(x_resampled)

"""Scaling fitur adalah langkah penting dalam machine learning, terutama untuk algoritma yang sensitif terhadap skala data (misalnya, KNN, SVM). Setelah oversampling, scaling dengan MinMaxScaler dalam rentang (-1, 1) memberikan manfaat berikut:

- **Normalisasi**: Menyamakan skala semua fitur agar tidak ada fitur yang mendominasi.
- **Konvergensi Lebih Cepat**: Mempercepat proses optimasi pada algoritma berbasis gradien.
- **Konsistensi**: Memastikan data asli dan sintetis memiliki skala yang sama.

Langkah ini penting untuk menjaga kualitas data setelah oversampling.

## Modelling

### Dengan pendekatan machine learning
"""

parameters = {
    'RandomForestClassifier': {
        'n_estimators': [50, 100,150, 170,200,230,250,300]
    },

    'KNeighborsClassifier': {
        'n_neighbors': [3,5,7,10,15,20,30],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean', 'manhattan']
    },

    'XGBClassifier': {
        'learning_rate': [0.001,0.01, 0.1, 0.5,1],
        'n_estimators': [50, 100,150, 200,250,350,300,350]
    }
}

"""Parameter grid didefinisikan untuk beberapa algoritma seperti RandomForestClassifier, KNeighborsClassifier, dan XGBClassifier. Parameter ini digunakan untuk mencari kombinasi terbaik melalui proses Grid Search."""

models = {
    'RandomForestClassifier': RandomForestClassifier(),
    'KNeighborsClassifier': KNeighborsClassifier(),
    'XGBClassifier': XGBClassifier()
}

"""Model-model yang akan diuji didefinisikan dalam dictionary, seperti RandomForestClassifier, KNeighborsClassifier, dan XGBClassifier"""

results =list()
for model_name, model in models.items():
    print(f"Grid searching for {model_name}")
    param_grid = parameters[model_name]
    grid_search = GridSearchCV(model, param_grid, cv=4, scoring='accuracy')
    grid_search.fit(x_resampled,y_resampled)
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_accuracy = grid_search.best_score_

    results.append({'Model': model_name, 'Best Params': best_params, 'Best Accuracy': best_accuracy})
results_df = pd.DataFrame(results)

"""Grid Search untuk Hyperparameter Tuning:
- Proses Grid Search dilakukan untuk mencari kombinasi parameter terbaik untuk setiap model.
- Proses ini menggunakan validasi silang (cv=4) dan metrik evaluasi berupa akurasi (scoring='accuracy')
"""

results_df.head(3)

models = {
    'RandomForestClassifier': RandomForestClassifier(n_estimators=150),
    'KNeighborsClassifier': KNeighborsClassifier(metric='euclidean',n_neighbors=30,weights='distance'),
    'XGBClassifier': XGBClassifier(learning_rate=0.1,n_estimators=250)
}

scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score)
}

scores = []
for model_name, model in models.items():
    print("-" * 50)
    print(f'{model_name}:')

    # Perform cross-validation
    cv_results = cross_validate(model, x, y, cv=4, scoring=scoring)

    # Collect and display the results
    accuracy = np.mean(cv_results['test_accuracy'])
    precision = np.mean(cv_results['test_precision'])
    recall = np.mean(cv_results['test_recall'])
    f1 = np.mean(cv_results['test_f1'])

    # Tampilkan hasil untuk model saat ini
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Collect the results for DataFrame
    model_results = {
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1': f1
    }
    scores.append(model_results)

# Create DataFrame from results
scores_df = pd.DataFrame(scores)

# Tampilkan hasil dalam bentuk DataFrame terformat
print("\n" + "="*50)
print("RINGKASAN HASIL EVALUASI MODEL:")
print("="*50)
print(scores_df.set_index('Model').round(4))

"""### Evaluasi Model dengan Cross-Validation

Pada tahap ini, dilakukan evaluasi terhadap beberapa model menggunakan teknik **Cross-Validation**. Berikut adalah langkah-langkah yang dilakukan:

**Definisi Model**  
   - Tiga model yang dievaluasi adalah:
     - `RandomForestClassifier` dengan `n_estimators=150`
     - `KNeighborsClassifier` dengan `metric='euclidean'`, `n_neighbors=30`, dan `weights='distance'`
     - `XGBClassifier` dengan `learning_rate=0.1` dan `n_estimators=250`

**Definisi Metrik Evaluasi**  
   - Metrik yang digunakan untuk mengevaluasi performa model adalah:
     - **Accuracy**: Proporsi prediksi yang benar terhadap total data.
     - **Precision**: Proporsi prediksi positif yang benar terhadap total prediksi positif.
     - **Recall**: Proporsi prediksi positif yang benar terhadap total data positif sebenarnya.
     - **F1 Score**: Harmonik rata-rata antara Precision dan Recall.

**Cross-Validation**  
   - Dilakukan **4-fold Cross-Validation** untuk setiap model. Data dibagi menjadi 4 bagian, di mana 3 bagian digunakan untuk pelatihan dan 1 bagian untuk pengujian, secara bergantian.

**Kesimpulan**:  
   Dari hasil evaluasi, model `RandomForestClassifier` memiliki performa terbaik berdasarkan metrik **Accuracy** (60.62%) dibandingkan model lainnya.
"""

scores_df.head(3)

"""## Pendekatan Deep Learning

Membangun Kompilasi Model
"""

# Define the ModelCheckpoint callback
checkpoint_callback = ModelCheckpoint(
    filepath='model_checkpoint.weights.h5',
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)
# Define the input layer
input_layer = Input(shape=(9,))
layer_1=Dense(64,activation='relu')(input_layer)
layer_2=Dense(64,activation='relu')(layer_1)
layer_3=Dense(64,activation='relu')(layer_2)
layer_4=Dropout(0.2)(layer_3)
layer_5=Dense(16,activation='relu')(layer_4)
output_layer=Dense(1,activation='sigmoid')(layer_5)
model=Model(inputs=input_layer,outputs=output_layer)

model.compile(optimizer="adam",
              loss='binary_crossentropy',
              metrics=['accuracy','precision', 'recall'])

"""Pada tahap ini, model deep learning didefinisikan menggunakan **Keras Functional API**. Berikut adalah arsitektur model yang digunakan:
- **Input Layer**: Menerima input dengan dimensi `(9,)`, sesuai dengan jumlah fitur pada dataset.
- **Hidden Layers**:
  - Tiga hidden layer pertama menggunakan **Dense Layer** dengan 64 neuron dan fungsi aktivasi `ReLU`.
  - Satu **Dropout Layer** dengan rate 0.2 untuk mencegah overfitting.
  - Satu hidden layer tambahan dengan 16 neuron dan fungsi aktivasi `ReLU`.
- **Output Layer**: Menggunakan **Dense Layer** dengan 1 neuron dan fungsi aktivasi `Sigmoid` untuk menghasilkan output probabilitas (karena ini adalah masalah binary classification).

#### Proses Training Model
"""

history=model.fit(
    x_resampled, y_resampled,
    validation_split=0.2,
    epochs=200,
    batch_size=64,
    verbose=1,
    callbacks=[checkpoint_callback]
)

"""### Evaluasi Model"""

model=load_model('/content/model_checkpoint.weights.h5')

model.summary()

"""Model ini memiliki arsitektur yang sederhana namun cukup kuat untuk tugas klasifikasi biner. Dengan total 10,019 parameter, model ini dirancang untuk mempelajari pola dari dataset dengan 9 fitur input dan menghasilkan probabilitas untuk menentukan apakah air layak minum atau tidak."""

x_train,x_test,y_train,y_test=train_test_split(x_resampled,y_resampled,test_size=0.2)

model.evaluate(x_test,y_test)

"""**Penjelasan**
- Model memiliki performa yang sangat baik pada dataset uji, dengan akurasi tinggi (97%), precision tinggi (96%), dan recall tinggi (98%).
- Nilai loss yang rendah (`0.1510`) menunjukkan bahwa model tidak mengalami overfitting atau underfitting yang signifikan.
- Dengan precision dan recall yang tinggi, model ini sangat andal untuk tugas klasifikasi biner, khususnya dalam menentukan apakah air layak minum atau tidak.
"""

plt.figure(figsize=(8,8))
plt.subplot(3, 1, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.subplot(3, 1, 2)
plt.plot(history.history['precision'])
plt.plot(history.history['val_precision'])
plt.title('Model Precision')
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.subplot(3, 1, 3)
plt.plot(history.history['recall'])
plt.plot(history.history['val_recall'])
plt.title('Model Recall')
plt.xlabel('Epoch')
plt.ylabel('Recall')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.tight_layout()
plt.show()

"""### Kesimpulan

- **Performa Model**: Model menunjukkan performa yang baik pada data pelatihan, dengan metrik (accuracy, precision, recall) yang meningkat secara konsisten.

- **Generalization**: Meskipun performa pada data validasi cukup baik, fluktuasi pada grafik validasi (terutama pada precision dan recall) menunjukkan bahwa model mungkin mengalami sedikit overfitting pada data pelatihan.

- **Rekomendasi**: Untuk mengurangi fluktuasi dan meningkatkan generalisasi, beberapa langkah dapat dipertimbangkan:
  - Menambahkan regularisasi (seperti Dropout tambahan atau L2 regularization).
  - Menggunakan teknik early stopping untuk menghentikan pelatihan sebelum model mulai overfit.
  - Meningkatkan ukuran dataset atau menggunakan augmentasi data jika memungkinkan.
"""

# Import library yang diperlukan
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Prediksi pada data test
y_pred_raw = model.predict(x_test)
# Konversi ke label biner (karena output sigmoid)
y_pred = (y_pred_raw > 0.5).astype(int)

# Hitung confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Ekstrak nilai TP, TN, FP, FN
TN, FP, FN, TP = cm.ravel()

# Visualisasi confusion matrix dengan anotasi yang jelas
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)

# Tambahkan label yang jelas
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('True Label', fontsize=12)
plt.title('Confusion Matrix', fontsize=14)

plt.tight_layout()
plt.show()

"""
Confusion matrix di atas menunjukkan hasil evaluasi model klasifikasi biner. Berikut adalah rincian nilai-nilai dalam matriks:

- **True Positive (TP):** 394  
  Model memprediksi kelas positif (1) dengan benar.

- **True Negative (TN):** 385  
  Model memprediksi kelas negatif (0) dengan benar.

- **False Positive (FP):** 5  
  Model memprediksi kelas positif (1), tetapi sebenarnya kelas negatif (0).

- **False Negative (FN):** 16  
  Model memprediksi kelas negatif (0), tetapi sebenarnya kelas positif (1).
"""

accuracy = metrics.accuracy_score(y_test, y_pred)
precision = metrics.precision_score(y_test, y_pred, average='weighted')
recall = metrics.recall_score(y_test, y_pred, average='weighted')
f1 = metrics.f1_score(y_test, y_pred, average='weighted')

classification_report = metrics.classification_report(y_test, y_pred)
print(f"Classification Report :\n{classification_report}")